# Model arguments
model_name_or_path: Qwen/Qwen3-1.7B
model_revision: main
torch_dtype: bfloat16
attn_implementation: flash_attention_2

# Data training arguments
dataset_name: watermelonhjg/MATH-lighteval-level_3
dataset_config: default
dataset_prompt_column: problem
dataset_train_split: train
dataset_test_split: validation
system_prompt: |
  You are a helpful AI Assistant that provides well-reasoned and detailed responses.
  First think about the necessary reasoning process as an internal monologue and then provide the user with the answer.
  Respond in the following format:
  <think>
  ...
  </think>
  <answer>
  ...
  </answer>
shuffle_dataset: false

# ADPO trainer config
anchor_update_mode: on_policy
beta_anchor_kl: 0
bf16: true
do_eval: true
tau: 0.8
beta_reward: 0.8
use_q_centering: true
use_adaptive_tau: true
adaptive_tau_alpha: 1.0
adaptive_tau_min: 0.3
use_vllm: true
vllm_mode: colocate
vllm_enable_sleep_mode: true
vllm_gpu_memory_utilization: 0.4
gradient_accumulation_steps: 16
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
hub_model_id: Qwen3-1.7B-Open-R1-ADPO
hub_strategy: every_save
learning_rate: 1.5e-05
log_completions: true
log_level: info
logging_first_step: true
logging_steps: 1
logging_strategy: steps
lr_scheduler_type: cosine
max_prompt_length: 800
max_completion_length: 1024
max_steps: -1
num_generations: 8
num_train_epochs: 4
dataloader_drop_last: true
output_dir: data/Qwen3-1.7B-Open-R1-ADPO
overwrite_output_dir: true
per_device_eval_batch_size: 8
per_device_train_batch_size: 8
push_to_hub: true
report_to:
  - wandb
reward_funcs:
  - good_accuracy
reward_weights:
  - 1.0
save_strategy: "epoch"
save_total_limit: 1
seed: 42
warmup_ratio: 0.1

# WandB logging
wandb_project: open-r1-ADPO
wandb_run_group: qwen3_adpo_baseline
wandb_entity: null
run_name: qwen3-1.7b-adpo-baseline
