# Model arguments
model_name_or_path: Qwen/Qwen3-1.7B
model_revision: main
torch_dtype: bfloat16
attn_implementation: flash_attention_2

# Data training arguments
dataset_name: wzx111/MATH-lighteval-level-middlehigh
dataset_config: default
dataset_prompt_column: problem
dataset_train_split: train
dataset_test_split: validation
system_prompt: |
  You are a helpful AI Assistant that provides well-reasoned and detailed responses.
  First think about the reasoning process as an internal monologue and then provide the user with the answer.
  Respond in the following format:
  <think>
  ...
  </think>
  <answer>
  ...
  </answer>
shuffle_dataset: true

# ADPO trainer config (Server mode)
anchor_update_mode: on_policy
beta_anchor_kl: 0
bf16: true
do_eval: true
drop_all_failed_prompts: true
tau: 1.0
beta_reward: 0.8
use_q_centering: true
use_adaptive_tau: true
adaptive_tau_alpha: 0.7
adaptive_tau_min: 0.3
use_vllm: true
vllm_mode: server
vllm_server_base_url: http://127.0.0.1:8000

# Reward function parameters
repetition_n_grams: 3
repetition_max_penalty: -1.0

gradient_accumulation_steps: 4
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
hub_model_id: Qwen3-1.7B-Open-R1-ADPO-Server
hub_strategy: every_save
learning_rate: 1.5e-05
log_completions: true
log_level: info
logging_first_step: true
logging_steps: 1
logging_strategy: steps
lr_scheduler_type: cosine
max_prompt_length: 800
max_completion_length: 1400
max_steps: -1
num_generations: 8
num_train_epochs: 2
dataloader_drop_last: true
output_dir: data/Qwen3-1.7B-Open-R1-ADPO-Server
overwrite_output_dir: true
per_device_eval_batch_size: 8
per_device_train_batch_size: 8
push_to_hub: true
report_to:
  - wandb
reward_funcs:
  - good_accuracy
reward_weights:
  - 1.0
save_strategy: "epoch"
save_total_limit: 1
seed: 42
warmup_ratio: 0.1
mask_truncated_completions: true 
# WandB logging
wandb_project: open-r1-ADPO
wandb_run_group: qwen3_adpo_server
wandb_entity: null
run_name: qwen3-1.7b-adpo-server
