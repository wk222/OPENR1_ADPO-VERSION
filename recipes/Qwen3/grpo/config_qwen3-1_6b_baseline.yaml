# Model arguments
model_name_or_path: Qwen/Qwen3-1.7B
model_revision: main
torch_dtype: bfloat16
attn_implementation: flash_attention_2

# Data training arguments
dataset_name: wzx111/MATH-lighteval-level-middlehigh
dataset_config: default
dataset_prompt_column: problem
dataset_train_split: train
dataset_test_split: validation
system_prompt: |
  You are a helpful AI Assistant that provides well-reasoned and detailed responses.
  First think about the necessary reasoning process as an internal monologue and then provide the user with the answer.
  Respond in the following format:
  <think>
  ...
  </think>
  <answer>
  ...
  </answer>
shuffle_dataset: true

# GRPO trainer config (Baseline)
bf16: true
do_eval: true
use_vllm: true
vllm_mode: colocate
vllm_enable_sleep_mode: true
vllm_gpu_memory_utilization: 0.5

# GSPO / Sequence-level config
# 设置为 sequence 级别，使用句子概率
importance_sampling_level: sequence

# 关键点：beta=0 关闭 KL 惩罚，纯靠 Clipping
beta: 0.0
epsilon: 0.2

# Reward function parameters
repetition_n_grams: 3
repetition_max_penalty: -1.0

gradient_accumulation_steps: 4
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
hub_model_id: Qwen3-1.7B-Open-R1-GRPO-Baseline
hub_strategy: every_save
learning_rate: 1.5e-05
log_completions: true
log_level: info
logging_first_step: true
logging_steps: 1
logging_strategy: steps
lr_scheduler_type: cosine
max_prompt_length: 800
max_completion_length: 1400
max_steps: -1
num_generations: 8
num_train_epochs: 2
dataloader_drop_last: true
output_dir: data/Qwen3-1.7B-Open-R1-GRPO-Baseline
overwrite_output_dir: true
per_device_eval_batch_size: 8
per_device_train_batch_size: 8
push_to_hub: true
report_to:
  - wandb
reward_funcs:
  - good_accuracy
reward_weights:
  - 1.0
save_strategy: "epoch"
save_total_limit: 1
seed: 42
warmup_ratio: 0.1

# WandB logging
wandb_project: open-r1-ADPO
wandb_run_group: qwen3_grpo_baseline
wandb_entity: null
run_name: qwen3-1.7b-grpo-baseline
