# Model arguments
model_name_or_path: Qwen/Qwen3-1.7B
model_revision: main
torch_dtype: bfloat16
attn_implementation: flash_attention_2

# Data training arguments
dataset_name: watermelonhjg/MATH-lighteval-level_3
dataset_config: default
dataset_prompt_column: problem
dataset_train_split: train
dataset_test_split: validation
system_prompt: |
  You are a helpful AI Assistant that provides well-reasoned and detailed responses.
  First think about the reasoning process as an internal monologue and then provide the user with the answer.
  Respond in the following format:
  <think>
  ...
  </think>
  <answer>
  ...
  </answer>
shuffle_dataset: false

# GRPO trainer config (Server mode)
bf16: true
do_eval: true
use_vllm: true
vllm_mode: server
vllm_server_base_url: http://127.0.0.1:8001

# GSPO / Sequence-level config
# 设置为 sequence 级别，使用句子概率
importance_sampling_level: sequence

# 关键点：beta=0 关闭 KL 惩罚，纯靠 Clipping
beta: 0.0
epsilon: 0.2

gradient_accumulation_steps: 16
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
hub_model_id: Qwen3-1.7B-Open-R1-GRPO-Server
hub_strategy: every_save
learning_rate: 1.5e-05
log_completions: true
log_level: info
logging_first_step: true
logging_steps: 1
logging_strategy: steps
lr_scheduler_type: cosine
max_prompt_length: 512
max_completion_length: 1024
max_steps: -1
num_generations: 8
num_train_epochs: 2
dataloader_drop_last: true
output_dir: data/Qwen3-1.7B-Open-R1-GRPO-Server
overwrite_output_dir: true
per_device_eval_batch_size: 8
per_device_train_batch_size: 8
push_to_hub: true
report_to:
  - wandb
reward_funcs:
  - good_accuracy
reward_weights:
  - 1.0
save_strategy: "epoch"
save_total_limit: 2
seed: 42
warmup_ratio: 0.1

# WandB logging
wandb_project: open-r1-ADPO
wandb_run_group: qwen3_grpo_server
wandb_entity: null
run_name: qwen3-1.7b-grpo-server
